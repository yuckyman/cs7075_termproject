```
gesture_pid/
├── config/
│   └── settings.py
├── docs/
│   └── HAND_GESTURES.md
├── src/
│   ├── gestures/
│   │   └── hand_tracker.py
│   ├── multimodal/
│   │   └── gesture_module.py
│   └── simulation/
│       ├── kuka_gesture_control.py
│       ├── sim_env.py
│       └── test_sim.py
├── tests/
├── venv/
├── requirements.txt
└── README.md

# Kuka Gesture Control Implementation

## Overview
- Connected MediaPipe-based gesture recognition to PyBullet Kuka arm simulation
- Implemented real-time gesture-to-motion mapping
- Added inverse kinematics for smooth arm movement

## Gesture Mapping
- `<gesture_point>` → lift end effector (z+)
- `<gesture_swipe>` → move right (x+)
- `<gesture_circle>` → move forward (y+)

## Detection Parameters
- MediaPipe confidence thresholds:
  - Detection: 0.5
  - Tracking: 0.3
- Gesture parameters:
  - Trail length: 15 frames
  - Min swipe distance: 0.1
  - Min circle points: 10
  - Circle threshold: 0.6

## Next Steps
- [ ] Add motion smoothing/interpolation
- [ ] Implement gesture cooldown to prevent jitter
- [ ] Add more gestures (fist for reset, open-palm for grab/release)
- [ ] Tune PID control for smoother motion
- [ ] Add workspace boundary visualization
