# # 2025-04-08 notes
## ## azure vm for model training
- connect to vm using ssh (azureuser@68.154.121.28)


let's talk about creating embeddings for these gestures. embeddings are basically a way to represent gestures as numerical vectors, which can then be used by machine learning models. here's a step-by-step plan to create these embeddings:

1. **data collection**: 
   - gather a dataset of videos or images that capture the gestures. you can use datasets like CMU panoptic or MPII human pose for general body language.
   - make sure the dataset includes variations and nuances of each gesture.

2. **feature extraction**:
   - use pose estimation models like mediapipe or openpose to extract keypoints from the images or videos. these keypoints represent the positions of joints and other important parts of the body.
   - for fine-grained gestures, focus on specific keypoints (e.g., hand, head, torso) to capture nuances.

3. **dimensionality reduction**:
   - apply techniques like PCA (Principal Component Analysis) or t-SNE (t-distributed Stochastic Neighbor Embedding) to reduce the dimensionality of the keypoint data. this helps in capturing the most important features while reducing noise.

4. **normalization**:
   - normalize the vectors using techniques like L2 normalization to ensure that all embeddings have the same scale. this is important for consistent comparison and integration with other data.

5. **embedding creation**:
   - map the reduced and normalized vectors to a fixed-size embedding space. this can be done using neural networks or other machine learning models.
   - ensure that similar gestures have similar embeddings, while distinct gestures have distinct embeddings.

6. **integration with LLM**:
   - once the embeddings are ready, integrate them with the language model by concatenating gesture tokens with text input.
   - adjust prompts to include nonverbal context, allowing the LLM to consider both verbal and nonverbal cues.

7. **testing and refinement**:
   - conduct real-time testing with video input to ensure that the embeddings accurately represent the gestures.
   - refine the mapping if certain gestures aren't recognized well or if the embeddings don't capture the nuances effectively.

this process will help us create robust gesture embeddings that can be used to enhance the interaction with the LLM. let me know if you want to dive deeper into any of these steps!
